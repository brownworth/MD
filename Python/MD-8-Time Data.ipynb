{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-based Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson, we'll be working with some of the ways that Python/Pandas can manipulate data based upon a time index. But, like everything we've done, it doesn't always start out the way we want it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two links are the data, and a README file that describes the data format. To keep it somewhat close-to-home, the data contained in the first link is from Durham, NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01/2018/CRNS0101-05-2018-NC_Durham_11_W.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking on that link, you'll see that there is a bunch of columns, but no headers. It's divided into fixed-width columns, but not with commas, or other single characters. Let's see what the pandas default does with this kind of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(r'./CRNS0101-05-2018-NC_Durham_11_W.txt').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not all that useful. Let's see what the README has to say about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll down to section 5, you'll see a bit where it describes the columns, but it's in what is essentially a text file. Can we get that into something we'd like? Of course! With Regular Expressions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Weather Headers](weather_headers.png \"Weather Headers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this list, it's pretty easy to see the information we need, and we should be able to formulate a regular expression to extract the middle column. Let's select the info in the browser; copy and paste into our text editor. We'll work in there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = pd.read_csv(r'./weather_headers.csv',header=None,squeeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the file we just created, but add a new keyword argument, \"squeeze\". This allows the imported single column to be treated as a pandas Series, rather than a pandas DataFrame. This affects the formatting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll import the data from the text file doing the following:\n",
    "* split on another regular expression, specifically `\\s+` meaning one or more whitespace characters.\n",
    "* treat the first line as data.\n",
    "* use the 'headers' from above as the headers of the columns\n",
    "* combine the local date and time into a single datetime field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data = pd.read_csv(r'./CRNS0101-05-2018-NC_Durham_11_W.txt',delimiter='\\s+',header=None,names=headers.values,parse_dates=[['LST_DATE','LST_TIME']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the data, this looks useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these samples occur about every 5 minutes, we'll change the index to be the datetime. This will allow for some other functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.set_index('LST_DATE_LST_TIME',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.groupby()` method, and an aggregate function, we can start to see some grouped data. We're also introducing `pd.Grouper`, a method for grouping by characteristics such as time. In this case, Pandas will take the data separated by 5 minute increments, group by some time frequency, and then apply a function to that group. Here we're grouping by week and getting the average air temperature for that week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.loc[:,'AIR_TEMPERATURE'].groupby(pd.Grouper(freq='w')).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is going on in April, May, or June. Is it really that close to absolute zero at the end of spring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data[(noaa_data.index >= '2018-05-27') & (noaa_data.index < '2018-06-03')].loc[:,'AIR_TEMPERATURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the list of data, we're not seeing anything out of the ordinary. Let's mask the data for values less than absolute zero, when applied to air temperature, and use `np.unique()` to get the date(s) associated with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(noaa_data[noaa_data['AIR_TEMPERATURE'] < -273.15]['AIR_TEMPERATURE'].index.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three days in 2018 had air temperatures lower than absolute zero. I think that would have made news. Let's look at a histogram to see what our distribution is for temperatures like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.histogram(noaa_data[(noaa_data.index >= '2018-05-29') & (noaa_data.index < '2018-05-30')].loc[:,'AIR_TEMPERATURE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while we're at it, let's transition to our favorite thing: indexers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.histogram(noaa_data.loc[\"2018-05-29\":\"2018-05-29\",\"AIR_TEMPERATURE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these bins, we can see a really weird distribution. Most of the data is in the rightmost bin, with temperatures being at or below 26.9 degrees celcius. But there are a lot of -9999 values. We know this to be incorrect data. In fact, this is indicated in the notes of our specification document:\n",
    "* C.  Missing data are indicated by the lowest possible integer for a given column format, such as -9999.0 for 7-character fields with one decimal place or -99.000 for 7-character fields with three decimal places.\n",
    "\n",
    "We don't always have specifications for errors, so it's good to have a couple of ways to look at where some outliers might make our data messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data['AIR_TEMPERATURE'].replace(-9999,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll replace the invalid data with `np.nan`. Even though `np.nan` is invalid data, it give some indication to functions that it should be omitted. Check out this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noaa_data.mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `.mean()` skips null/nan values. Look at how the following three examples work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3,np.nan]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3,np.nan]).mean(skipna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reapply based upon our fixed data. In theory, the values should be more in accordance with our expectations. We're dropping `np.nan` from our calculated mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.loc[:,'AIR_TEMPERATURE'].groupby(pd.Grouper(freq='W')).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But be careful, now that we have `np.nan` in our data, the histogram from above might be broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(noaa_data.loc[\"2018-05-29\":\"2018-05-29\",\"AIR_TEMPERATURE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explicitly use the `np.nanmin()` and `np.nanmax()` functions to find the minimum and maximums for a range, ignoring NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_day = noaa_data.loc[\"2018-05-29\":\"2018-05-29\",\"AIR_TEMPERATURE\"]\n",
    "np.histogram(noaa_day,range=(np.nanmin(noaa_day),np.nanmax(noaa_day)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of using the datetime values as an index, is that we can groupby properties of those dates. Ever need to look at data grouped by hour of the day? What was the average temperature for each hour in the month of July?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noaa_month = noaa_data.loc[\"2018-07-01\":\"2018-08-01\"]\n",
    "noaa_month.loc[:,'AIR_TEMPERATURE'].groupby(noaa_month.index.hour).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the hottest average day of the year (so far)? (0 = Monday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data['AIR_TEMPERATURE'].groupby(noaa_data.index.dayofweek).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to make it a little more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import day_name\n",
    "pd.DataFrame({'Day':[day_name[i] for i in range(7)],\n",
    "            'Avg. Temp':noaa_data['AIR_TEMPERATURE'].groupby(noaa_data.index.dayofweek).mean()},\n",
    "             columns=['Day','Avg. Temp']\n",
    ").set_index('Day').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at a situation where `.sum()` might be more appropriate, Precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.loc[:,'PRECIPITATION'].groupby(pd.Grouper(freq='W')).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still seeing the values affected by the invalid entries. Adding -9999 to a value potentially every 5 minutes can really throw off our analysis. We've seen how to replace a single value with `np.nan`, and that is almost certainly what we'll do here. However, if we're talking about an amount of precipitation in a 5 minute period, _any_ negative number could potentially be an invalid value. Let's investigate using ranges with start/end/steps as a way to leverage the power of replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_numbers = [10,8,3,1,5,-5,2,-15,-4,5,-2,-1,-3,-5]\n",
    "pd.Series(some_numbers).replace(range(-5,-1,2),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with visualization, let's put the results next to the original numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_numbers = [10,8,3,1,5,-5,2,-15,-4,5,-2,-1,-3,-5]\n",
    "new_numbers = pd.Series(old_numbers).replace(range(1,10,1),0)\n",
    "pd.concat([pd.Series(old_numbers),pd.Series(new_numbers)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've experimented with values in `.replace(range(x,y,z),n)` let's use that to change all negative numbers for precipitation to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data['PRECIPITATION'].replace(range(-9999,0),np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to verify that precipitation are discrete values, and not 5 minute cumulative numbers, let's take a look at a specific range to see how it behaves. We'll leverage some more functionality with indexers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.loc[\"2018-06-10 22:00:00\":\"2018-06-10\",'PRECIPITATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the indexers are smart enough to include time with the date. This segment was deliberately picked to show that it appears that the rainfall is per-five-minute-segment, instead of cumulative. That makes the following grouping using `.sum()` more likely to be a reasonable statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_data.loc[:,'PRECIPITATION'].groupby(pd.Grouper(freq='W')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
